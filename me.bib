@inproceedings{Deng2016,
abstract = {Building on recent advances in image caption generation and optical character recognition (OCR), we present a general-purpose, deep learning-based system to decompile an image into presentational markup. While this task is a well-studied problem in OCR, our method takes an inherently different, data-driven approach. Our model does not require any knowledge of the underlying markup language, and is simply trained end-to-end on real-world example data. The model employs a convolutional network for text and layout recognition in tandem with an attention-based neural machine translation system. To train and evaluate the model, we introduce a new dataset of real-world rendered mathematical expressions paired with LaTeX markup, as well as a synthetic dataset of web pages paired with HTML snippets. Experimental results show that the system is surprisingly effective at generating accurate markup for both datasets. While a standard domain-specific LaTeX OCR system achieves around 25{\%} accuracy, our model reproduces the exact rendered image on 75{\%} of examples.},
archivePrefix = {arXiv},
arxivId = {1609.04938},
author = {Deng, Yuntian and Kanervisto, Anssi and Rush, Alexander M.},
booktitle = {Arxiv},
eprint = {1609.04938},
month = {sep},
title = {{What You Get Is What You See: A Visual Markup Decompiler}},
url = {http://arxiv.org/abs/1609.04938},
year = {2016}
}
@inproceedings{Kim2016,
abstract = {We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our model employs a convolutional neural network (CNN) and a highway network over characters, whose output is given to a long short-term memory (LSTM) recurrent neural network language model (RNN-LM). On the English Penn Treebank the model is on par with the existing state-of-the-art despite having 60{\%} fewer parameters. On languages with rich morphology (Czech, German, French, Spanish, Russian), the model consistently outperforms a Kneser-Ney baseline and word-level/morpheme-level LSTM baselines, again with far fewer parameters. Our results suggest that on many languages, character inputs are sufficient for language modeling.},
archivePrefix = {arXiv},
arxivId = {1508.06615},
author = {Kim, Yoon and Jernite, Yacine and Sontag, David and Rush, Alexander M.},
booktitle = {AAAI},
eprint = {1508.06615},
title = {{Character-Aware Neural Language Models}},
url = {http://arxiv.org/abs/1508.06615},
year = {2016}
}
@inproceedings{deng2018latent,
  title={Latent alignment and variational attention},
    author={Deng, Yuntian and Kim, Yoon and Chiu, Justin and Guo, Demi and Rush, Alexander},
      booktitle={Advances in Neural Information Processing Systems},
        pages={9735--9747},
          year={2018}
          }

@inproceedings{senellart2018opennmt,
  title={OpenNMT System Description for WNMT 2018: 800 words/sec on a single-core CPU},
    author={Senellart, Jean and Zhang, Dakun and Bo, WANG and Klein, Guillaume and Ramatchandirin, Jean-Pierre and Crego, Josep and Rush, Alexander},
      booktitle={Proceedings of the 2nd Workshop on Neural Machine Translation and Generation},
        pages={122--128},
          year={2018}
          }

@inproceedings{Kim2016a,
abstract = {Neural machine translation (NMT) offers a novel alternative formulation of translation that is potentially simpler than statistical ap-proaches. However to reach competitive per-formance, NMT models need to be exceed-ingly large. In this paper we consider applying knowledge distillation approaches (Bucila et al., 2006; Hinton et al., 2015) that have proven successful for reducing the size of neural mod-els in other domains to the problem of NMT. We demonstrate that standard knowledge dis-tillation applied to word-level prediction can be effective for NMT, and also introduce two novel sequence-level versions of knowledge distillation that further improve performance, and somewhat surprisingly, seem to elimi-nate the need for beam search (even when ap-plied on the original teacher model). Our best student model runs 10 times faster than its state-of-the-art teacher with only a decrease of 0.2 BLEU. It is also significantly better than a baseline model trained without knowledge distillation: by 4.2/1.7 BLEU with greedy de-coding/beam search.},
archivePrefix = {arXiv},
arxivId = {1606.07947},
author = {Kim, Yoon and Rush, Alexander M.},
booktitle = {EMNLP},
eprint = {1606.07947},
file = {::},
month = {jun},
title = {{Sequence-Level Knowledge Distillation}},
url = {http://arxiv.org/abs/1606.07947},
year = {2016}
}
@inproceedings{Koo2010,
abstract = {This paper introduces algorithms for nonprojective parsing based on dual decomposition. We focus on parsing algorithms for nonprojective head automata, a generalization of head-automata models to non-projective structures. The dual decomposition algorithms are simple and efficient, relying on standard dynamic programming and minimum spanning tree algorithms. They provably solve an LP relaxation of the non-projective parsing problem. Empirically the LP relaxation is very often tight: for many languages, exact solutions are achieved on over 98{\%} of test sentences. The accuracy of our models is higher than previous work on a broad range of datasets.},
author = {Koo, Terry and Rush, Alexander M. and Collins, Michael and Jaakkola, Tommi and Sontag, David},
booktitle = {EMNLP},
keywords = {Dependency Parsing,Dual Decomposition,Non-Projective},
number = {October},
pages = {1288--1298},
title = {{Dual Decomposition for Parsing with Non-Projective Head Automata}},
year = {2010}
}
@inproceedings{Rush2015,
abstract = {Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines.},
archivePrefix = {arXiv},
arxivId = {1509.00685},
author = {Rush, Alexander M and Chopra, Sumit and Weston, Jason},
booktitle = {EMNLP},
eprint = {1509.00685},
isbn = {9781941643327},
number = {September},
pages = {379--389},
title = {{A Neural Attention Model for Abstractive Sentence Summarization}},
url = {http://arxiv.org/abs/1509.00685},
year = {2015}
}
@inproceedings{Rush2011,
abstract = {We describe an exact decoding algorithm for syntax-based statistical translation. The ap- proach uses Lagrangian relaxation to decom- pose the decoding problem into tractable sub- problems, thereby avoiding exhaustive dy- namic programming. Themethod recovers ex- act solutions, with certificates of optimality, on over 97{\%} of test examples; it has compa- rable speed to state-of-the-art decoders.},
author = {Rush, Alexander M and Collins, Michael},
booktitle = {ACL},
pages = {72--82},
title = {{Exact Decoding of Syntactic Translation Models through Lagrangian Relaxation}},
url = {http://www.aclweb.org/anthology/P11-1008},
year = {2011}
}
@article{Rush2012,
abstract = {Dual decomposition, and more generally Lagrangian relaxation, is a classical method for com- binatorial optimization; it has recently been applied to several inference problems in natural lan- guage processing (NLP). This tutorial gives an overview of the technique. We describe example al- gorithms, describe formal guarantees for the method, and describe practical issues in implementing the algorithms. While our examples are predominantly drawn from the NLP literature, the material should be of general relevance to inference problems in machine learning. A central theme of this tutorial is that Lagrangian relaxation is naturally applied in conjunction with a broad class of com- binatorial algorithms, allowing inference in models that go significantly beyond previous work on Lagrangian relaxation for inference in graphical models.},
author = {Rush, Alexander M. and Collins, Michael},
journal = {Journal of Artificial Intelligence Research},
pages = {305--362},
title = {{A tutorial on dual decomposition and lagrangian relaxation for inference in natural language processing}},
volume = {45},
year = {2012}
}
@inproceedings{Rush2010,
abstract = {This paper introduces dual decomposition as a framework for deriving inference algorithms for NLP problems. The approach relies on standard dynamic-programming algorithms as oracle solvers for sub-problems, together with a simple method for forcing agreement between the different oracles. The approach provably solves a linear programming (LP) relaxation of the global inference problem. It leads to algorithms that are simple, in that they use existing decoding algorithms; efficient, in that they avoid exact algorithms for the full model; and often exact, in that empirically they often recover the correct solution in spite of using an LP relaxation. We give experimental results on two problems: 1) the combination of two lexicalized parsing models; and 2) the combination of a lexicalized parsing model and a trigram part-of-speech tagger.},
author = {Rush, Alexander M. and Sontag, David and Collins, Michael and Jaakkola, Tommi},
booktitle = {EMNLP},
keywords = {Dual Decomposition,Linear Programming},
number = {October},
pages = {1--11},
title = {{On dual decomposition and linear programming relaxations for natural language processing}},
url = {http://dl.acm.org/citation.cfm?id=1870658.1870659},
year = {2010}
}
@inproceedings{Schmaltz2016,
abstract = {We demonstrate that an attention-based encoder-decoder model can be used for sentence-level grammatical error identification for the Automated Evaluation of Scientific Writing (AESW) Shared Task 2016. The attention-based encoder-decoder models can be used for the generation of corrections, in addition to error identification, which is of interest for certain end-user applications. We show that a character-based encoder-decoder model is particularly effective, outperforming other results on the AESW Shared Task on its own, and showing gains over a word-based counterpart. Our final model--a combination of three character-based encoder-decoder models, one word-based encoder-decoder model, and a sentence-level CNN--is the highest performing system on the AESW 2016 binary prediction Shared Task.},
archivePrefix = {arXiv},
arxivId = {1604.04677},
author = {Schmaltz, Allen and Kim, Yoon and Rush, Alexander M. and Shieber, Stuart M.},
booktitle = {arxiv},
eprint = {1604.04677},
title = {{Sentence-Level Grammatical Error Identification as Sequence-to-Sequence Correction}},
url = {http://arxiv.org/abs/1604.04677},
year = {2016}
}
@inproceedings{Strobelt2016,
abstract = {Recurrent neural networks, and in particular long short-term memory networks (LSTMs), are a remarkably effective tool for sequence modeling that learn a dense black-box hidden representation of their sequential input. Researchers interested in better understanding these models have studied the changes in hidden state representations over time and noticed some interpretable patterns but also significant noise. In this work, we present LSTMVis a visual analysis tool for recurrent neural networks with a focus on understanding these hidden state dynamics. The tool allows a user to select a hypothesis input range to focus on local state changes, to match these states changes to similar patterns in a large data set, and to align these results with domain specific structural annotations. We further show several use cases of the tool for analyzing specific hidden state properties on datasets containing nesting, phrase structure, and chord progressions, and demonstrate how the tool can be used to isolate patterns for further statistical analysis.},
archivePrefix = {arXiv},
arxivId = {1606.07461},
author = {Strobelt, Hendrik and Gehrmann, Sebastian and Huber, Bernd and Pfister, Hanspeter and Rush, Alexander M.},
booktitle = {Arxiv},
eprint = {1606.07461},
file = {::},
month = {jun},
title = {{Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1606.07461},
year = {2016}
}
@inproceedings{Weston2015,
abstract = {One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks.},
archivePrefix = {arXiv},
arxivId = {1502.05698},
author = {Weston, Jason and Bordes, Antoine and Chopra, Sumit and Mikolov, Tomas and Rush, Alexander M.},
booktitle = {arXiv preprint},
doi = {10.1016/j.jpowsour.2014.09.131},
eprint = {1502.05698},
isbn = {1502.05698},
issn = {1502.05698},
keywords = {I,boring formatting information,machine learning},
title = {{Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks}},
url = {http://arxiv.org/abs/1502.05698},
year = {2015}
}
@inproceedings{Wiseman2015,
abstract = {We introduce a simple, non-linear mention-ranking model for coreference resolution that attempts to learn distinct feature representations for anaphoricity detection and antecedent ranking, which we encourage by pre-training on a pair of corresponding subtasks. Although we use only simple, unconjoined features, the model is able to learn useful representa- tions, and we report the best overall score on the CoNLL 2012 English test set to date.},
author = {Wiseman, Sam and Rush, Alexander M and Shieber, Stuart M and Weston, Jason},
booktitle = {ACL},
pages = {1416--1426},
title = {{Learning Anaphoricity and Antecedent Ranking Features for Coreference Resolution}},
year = {2015}
}
@inproceedings{Rush,
abstract = {Coarse-to-fine inference has been shown to be a robust approximate method for improving the efficiency of structured prediction models while preserving their accuracy. We propose a multi-pass coarse-to-fine architecture for de-pendency parsing using linear-time vine prun-ing and structured prediction cascades. Our first-, second-, and third-order models achieve accuracies comparable to those of their un-pruned counterparts, while exploring only a fraction of the search space. We observe speed-ups of up to two orders of magnitude compared to exhaustive search. Our pruned third-order model is twice as fast as an un-pruned first-order model and also compares favorably to a state-of-the-art transition-based parser for multiple languages.},
author = {Rush, Alexander M and Petrov, Slav},
booktitle = {NAACL},
file = {:home/srush/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rush, Petrov - Unknown - Vine Pruning for Efficient Multi-Pass Dependency Parsing.pdf:pdf},
title = {{Vine Pruning for Efficient Multi-Pass Dependency Parsing}},
year = {2012}
}
@inproceedings{Wiseman2016a,
abstract = {Sequence-to-Sequence (seq2seq) modeling has rapidly become an important general-purpose NLP tool that has proven effective for many text-generation and sequence-labeling tasks. Seq2seq builds on deep neural language modeling and inherits its remarkable accuracy in estimating local, next-word distributions. In this work, we introduce a model and beam-search training scheme, based on the work of Daume III and Marcu (2005), that extends seq2seq to learn global sequence scores. This structured approach avoids classical biases associated with local training and unifies the training loss with the test-time usage, while preserving the proven model architecture of seq2seq and its efficient training approach. We show that our system outperforms a highly-optimized attention-based seq2seq system and other baselines on three different sequence to sequence tasks: word ordering, parsing, and machine translation.},
archivePrefix = {arXiv},
arxivId = {1606.02960},
author = {Wiseman, Sam and Rush, Alexander M.},
booktitle = {EMNLP},
eprint = {1606.02960},
file = {::},
month = {jun},
title = {{Sequence-to-Sequence Learning as Beam-Search Optimization}},
url = {http://arxiv.org/abs/1606.02960},
year = {2016}
}
@inproceedings{Wiseman2016,
abstract = {There is compelling evidence that coreference prediction would benefit from modeling global information about entity-clusters. Yet, state-of-the-art performance can be achieved with systems treating each mention prediction independently, which we attribute to the inherent difficulty of crafting informative cluster-level features. We instead propose to use recurrent neural networks (RNNs) to learn latent, global representations of entity clusters directly from their mentions. We show that such representations are especially useful for the prediction of pronominal mentions, and can be incorporated into an end-to-end coreference system that outperforms the state of the art without requiring any additional search.},
archivePrefix = {arXiv},
arxivId = {1604.03035},
author = {Wiseman, Sam and Rush, Alexander M. and Shieber, Stuart M.},
booktitle = {NAACL},
eprint = {1604.03035},
title = {{Learning Global Features for Coreference Resolution}},
url = {http://arxiv.org/abs/1604.03035},
year = {2016}
}

@inproceedings{EMNLP2017,
abstract = {There is compelling evidence that coreference prediction would benefit from modeling global information about entity-clusters. Yet, state-of-the-art performance can be achieved with systems treating each mention prediction independently, which we attribute to the inherent difficulty of crafting informative cluster-level features. We instead propose to use recurrent neural networks (RNNs) to learn latent, global representations of entity clusters directly from their mentions. We show that such representations are especially useful for the prediction of pronominal mentions, and can be incorporated into an end-to-end coreference system that outperforms the state of the art without requiring any additional search.},
archivePrefix = {arXiv},
arxivId = {1604.03035},
author = {Wiseman, Sam and Rush, Alexander M. and Shieber, Stuart M.},
booktitle = {EMNLP},
eprint = {1604.03035},
title = {{Challenges in Data-to-Document Generation}},
url = {http://arxiv.org/abs/1604.03035},
year = {2017}
}

@ARTICLE{2017arXiv170604223J,
   author = {{Junbo}, Zhao and {Kim}, Yoon and {Zhang}, Kelly and {Rush}, Alexander.~M. and
          {LeCun}, Y.},
              title = "{Adversarially Regularized Autoencoders for Generating Discrete Structures}",
                journal = {ArXiv e-prints},
                archivePrefix = "arXiv",
                   eprint = {1706.04223},
                    primaryClass = "cs.LG",
                     keywords = {Computer Science - Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
                          year = 2017,
                              month = jun,
                                 adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170604223J},
                                   adsnote = {Provided by the SAO/NASA Astrophysics Data System}
                                   }
                                   
@article{strobelt2019s,
  title={Seq2seq-V is: A Visual Debugging Tool for Sequence-to-Sequence Models},
    author={Strobelt, Hendrik and Gehrmann, Sebastian and Behrisch, Michael and Perer, Adam and Pfister, Hanspeter and Rush, Alexander M},
      journal={IEEE transactions on visualization and computer graphics},
        volume=25,
          number=1,
            pages={353--363},
              year=2019,
                publisher={IEEE}
                }

@inproceedings{DBLP:conf/acl/KleinKDSR17,
  author    = {Guillaume Klein and
                 Yoon Kim and
                                Yuntian Deng and
                                               Jean Senellart and
                                                              Alexander M. Rush},
                                                                title     = {OpenNMT: Open-Source Toolkit for Neural Machine Translation},
                                                                  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational
                                                                                 Linguistics, {ACL} 2017, Vancouver, Canada, July 30 - August 4, System
                                                                                                Demonstrations},
                                                                                                  pages     = {67--72},
                                                                                                    year      = {2017},
                                                                                                      crossref  = {DBLP:conf/acl/2017-d},
                                                                                                        url       = {https://doi.org/10.18653/v1/P17-4012},
                                                                                                          doi       = {10.18653/v1/P17-4012},
                                                                                                            timestamp = {Fri, 04 Aug 2017 16:51:12 +0200},
                                                                                                              biburl    = {https://dblp.org/rec/bib/conf/acl/KleinKDSR17},
                                                                                                                bibsource = {dblp computer science bibliography, https://dblp.org}
                                                                                                                }


@inproceedings{DBLP:conf/emnlp/WisemanSR17,
  author    = {Sam Wiseman and
                 Stuart M. Shieber and
                                Alexander M. Rush},
                                  title     = {Challenges in Data-to-Document Generation},
                                    booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural
                                                   Language Processing, {EMNLP} 2017, Copenhagen, Denmark, September
                                                                  9-11, 2017},
                                                                    pages     = {2253--2263},
                                                                      year      = {2017},
                                                                        crossref  = {DBLP:conf/emnlp/2017},
                                                                          url       = {https://aclanthology.info/papers/D17-1239/d17-1239},
                                                                            timestamp = {Tue, 30 Jan 2018 13:42:04 +0100},
                                                                              biburl    = {https://dblp.org/rec/bib/conf/emnlp/WisemanSR17},
                                                                                bibsource = {dblp computer science bibliography, https://dblp.org}
                                                                                }


@inproceedings{DBLP:journals/corr/abs-1802-02550,
  author    = {Yoon Kim and
                 Sam Wiseman and
                                Andrew C. Miller and
                                               David Sontag and
                                                              Alexander M. Rush},
                                                                title     = {Semi-Amortized Variational Autoencoders},
                                                                  booktile   = {Proceedings of the 2018 International Conference of Machine Learning},
                                                                      year      = {2018},
                                                                        url       = {http://arxiv.org/abs/1802.02550},
                                                                            eprint    = {1802.02550},
                                                                              timestamp = {Fri, 02 Mar 2018 13:46:22 +0100},
                                                                                biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1802-02550},
                                                                                  bibsource = {dblp computer science bibliography, https://dblp.org}
                                                                                  }


@article{DBLP:journals/corr/KimDHR17,
  author    = {Yoon Kim and
                 Carl Denton and
                                Luong Hoang and
                                               Alexander M. Rush},
                                                 title     = {Structured Attention Networks},
                                                   booktitle   = {ICLR},
                                                     volume    = {abs/1702.00887},
                                                       year      = {2017},
                                                         url       = {http://arxiv.org/abs/1702.00887},
                                                           timestamp = {Wed, 07 Jun 2017 14:40:11 +0200},
                                                             biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/KimDHR17},
                                                               bibsource = {dblp computer science bibliography, http://dblp.org}
                                                               }


@article{reagen2017weightless,
  title={Weightless: Lossy Weight Encoding For Deep Neural Network Compression},
    author={Reagen, Brandon and Gupta, Udit and Adolf, Robert and Mitzenmacher, Michael M and Rush, Alexander M and Wei, Gu-Yeon and Brooks, David},
      journal={arXiv preprint arXiv:1711.04686},
        year={2017}
        }


@inproceedings{senellart2018opennmt,
  title={OpenNMT System Description for WNMT 2018: 800 words/sec on a single-core CPU},
    author={Senellart, Jean and Zhang, Dakun and Bo, WANG and Klein, Guillaume and Ramatchandirin, Jean-Pierre and Crego, Josep and Rush, Alexander},
      booktitle={Proceedings of the 2nd Workshop on Neural Machine Translation and Generation},
        pages={122--128},
          year={2018}
          }

@inproceedings{rush2018annotated,
  title={The Annotated Transformer},
    author={Rush, Alexander},
      booktitle={Proceedings of Workshop for NLP Open Source Software (NLP-OSS)},
        pages={52--60},
          year={2018}
          }


@article{wiseman2018learning,
  title={Learning neural templates for text generation},
    author={Wiseman, Sam and Shieber, Stuart M and Rush, Alexander M},
      journal={arXiv preprint arXiv:1808.10122},
        year=2018
        }