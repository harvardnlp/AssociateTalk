\begin{frame}{Outline}

  \begin{itemize}
  \item \textcolor{gray}{Model: Structure and Implementation}
    \air
  \item \textbf{Work 1: Rethinking Training (\textit{Beam Search Optimization})}
    \air

  \item \textcolor{gray}{Work 2: Rethinking Generation}
    \air

  \item \textcolor{gray}{Challenges: Text Generation and Deep Learning}
  \end{itemize}

  \begin{center}
    \textbf{Can we learn parameters $\theta$ to better target text generation
    applications?}
  \end{center}


\end{frame}


% \begin{frame}{Part 3: Structured Modeling}
%   \begin{center}
%     \scalebox{2}{
%   \begin{tikzpicture}

%     \node<1> at (10mm, 15mm){\includegraphics[width=3cm]{rnnlm6}};
%     \node[draw,  thick, rounded corners, scale=0.5] (ana) at (-15mm, 15mm) {Analysis};
%     \node[draw,  fill=yellow, thick, rounded corners,scale=0.5] (meth) at (0mm, 30mm) {\ Methods\phantom{p}};
%     \node[draw,  thick, rounded corners,scale=0.5] (app) at (25mm, 30mm) {Applications};
%     \node[draw, thick, rounded corners, scale=0.5] (und) at (35mm, 15mm) {Understanding};
%     \node[draw, fill=yellow, thick, rounded corners, scale=0.5] (dep) at (25mm, 0mm){Deployment};

%     \node[draw, thick, rounded corners,scale=0.5] (imp) at (0, 0) {Implementation};

%     \draw (ana) -- (meth) --(app) -- (und) -- (dep) -- (imp) -- (ana);

%   \end{tikzpicture}
% }
%   \end{center}
% \end{frame}


% \begin{frame}{Beam Search Optimization}

%     % Training Setup:
%     % \begin{itemize}
%     % \item $(x, \hat{y}_{1:T})$ - input, output sentence pair
%     % \item $\cal L(\theta)$ - loss function
%     % \item $f_{\theta}$ - learned scoring function
%     % \end{itemize}


%   % \begin{itemize}

%   % \item Input {$x$},  \textit{what to talk about}
%   %   \air

%   % \item Output text {$y^*_{1:T}$}, \textit{how to say it}
%   %   \air

%   % \item Scoring function {$f_\theta$}
%   % \end{itemize}

% \end{frame}


\begin{frame}{Baseline: Training Encoder-Decoder}
  Parameters $\theta$ are trained to score the next word given the \textit{true} history, ($\hat{y}_{1:t-1}$)

  \begin{center}
  \includegraphics[height=3.5cm, trim={12cm 3cm 7cm 0.5cm}, clip]{nmt-noattn-4}
  \includegraphics[height=3.5cm, trim={12cm 3cm 3cm 0.5cm}, clip]{nmt-noattn-6}
  \includegraphics[height=3.5cm, trim={12cm 3cm 0.5cm 0.5cm}, clip]{nmt-noattn-7}
  % \includegraphics[height=3.5cm, trim={12cm 3cm 0.5cm 0.5cm}, clip]{nmt-noattn-8}
  \end{center}

  Training loss is identical to \textcolor{redpurple}{multiclass classification},
  \[ {\cal L}(\theta) = -\sum_{t} \log p(\hat{y}_t\ |\  \hat{y}_{1:t-1}, x; \theta) \]
\end{frame}

\begin{frame}{Generating with Encoder-Decoder}
  Parameters $\theta$ are used to score the next word given \textit{any} history, ($y_{1:t-1}$)


  \begin{center}
  % \includegraphics[height=3.5cm, trim={12cm 3cm 7cm 0.5cm}, clip]{nmt-noattn-3}
  \includegraphics[height=3.5cm, trim={12cm 3cm 7cm 0.5cm}, clip]{nmt-noattn-4}
  \includegraphics[height=3.5cm, trim={12cm 3cm 0.5cm 0.5cm}, clip]{nmt-noattn-5}
  \includegraphics[width=7cm]{beam}
  \end{center}

  Generation aims to maximize over all sequences,
  \[ y^*_{1:T} =\argmax_{y_{1:T}} f_{\theta}(y_{1:T}, x) = \argmax_{y_{1:T}} \sum_{t} \log p(y_{t} | y_{1:t-1}, x; \theta) \]
  % \pause
  % Intractable to solve exactly $O(\text{\#vocab} ^T)$

\end{frame}

\begin{frame}[fragile]{Standard Heuristic Method:  Beam Search}
  \centerline{$ y^*_{1:T} \approx \argmax_{y_{1:T}} f_{\theta}(y_{1:T}, x)$ }

  \air
  \air

    \begin{center}
  \begin{tikzpicture}[transform canvas = {scale=0.8}]
    \tikzstyle{beam}=[draw, minimum height=0.6cm, anchor=base, text height=5, text depth=0, minimum width=1.5cm,thin, rounded corners, line width=0.03cm]
   \tikzstyle{mat}=[draw=white]
    \tikzset{>=stealth',every on chain/.append style={join},
      every join/.style={->}}


       \begin{scope}
   \matrix (G) [matrix of nodes, nodes={beam},inner sep=1mm,row sep=0.03cm, column sep=0.8cm ] {
    \node<1->(G-1-1){a}; & \node<2->(G-1-2){red}; & \node<3->(G-1-3){dog}; & \node<4->(G-1-4){smells}; & \node<5->(G-1-5){home};  & \node<6->(G-1-6){today}; \\
    \node<1->(G-2-1){the}; & \node<2->(G-2-2){dog}; & \node<3->(G-2-3){dog}; & \node<4->(G-2-4){barks}; & \node<5->(G-2-5){quickly}; & \node<6->(G-2-6){Friday}; \\
    \node<1->(G-3-1){red}; & \node<2->(G-3-2){blue}; & \node<3->(G-3-3){cat}; &  \node<4->(G-3-4){walks}; & \node<5->(G-3-5){straight}; & \node<6->(G-3-6){now}; \\    };

    \only<2->{
      \draw[->] (G-1-1.east) -> (G-1-2.west);
      \draw[->] (G-2-1.east) -> (G-2-2.west);
      \draw[->] (G-1-1.east) -> (G-3-2.west);
      \draw[double, line width=0.03cm] (G-3-1.south west) -- (G-3-2.south east);
    }

    \only<3->{
      \draw[->] (G-1-2.east) -> (G-2-3.west);
      \draw[->] (G-3-2.east) -> (G-3-3.west);
      \draw[->] (G-3-2.east) -> (G-1-3.west);
      \draw[double, line width=0.03cm] (G-3-1.south west) -- (G-3-3.south east);
    }

 \only<4->{
    \draw[->] (G-1-3.east) -> (G-3-4.west);
    \draw[->] (G-2-3.east) -> (G-2-4.west);
    \draw[->] (G-1-3.east) -> (G-1-4.west);
    \draw[double, line width=0.03cm] (G-3-1.south west) -- (G-3-4.south east);
}
 \only<6->{
    \draw[->] (G-1-5.east) -> (G-1-6.west);
    \draw[->] (G-1-5.east) -> (G-3-6.west);
    \draw[->] (G-2-5.east) -> (G-2-6.west);
    \draw[double, line width=0.03cm] (G-3-1.south west) -- (G-3-6.south east);
}

 \only<5->{
    \draw[->] (G-3-4.east) -> (G-1-5.west);
    \draw[->] (G-3-4.east) -> (G-2-5.west);
    \draw[->] (G-3-4.east) -> (G-3-5.west);
    \draw[double, line width=0.03cm] (G-3-1.south west) -- (G-3-5.south east);
}
         \node[yshift=0.7cm, left=0.2cm of G]{$y^{(1)}$};
         \node(q)[left=0.2cm of G]{$y^{(2)}$};
         \node[left=0.2cm of q]{$k$};
         \node[yshift=-0.7cm, left=0.2cm of G]{$y^{(3)}$};
         \node[below=0.1cm of G]{t};

\end{scope}
\end{tikzpicture}
    \end{center}

\air
    \air

\air

\visible<7->{
  Start with $K$ sequence in beam.

   \begin{enumerate}
   \item For each $k$ in beam, expand and score all possible next words $y_t$.

   \item Prune all expansions ($K \times \text{vocab}$) down to top $K$.
   \end{enumerate}
}

  % \begin{enumerate}
  % \item Start with $K$ partial starting hypotheses $\wvec^{(1:K)}$
  % \item For timesteps $t$ from  $1$ to $T$:
  %  \begin{enumerate}
  %  \item Compute for all $k, \wvec_{t}$
  %    \[s(\wvec_t, k) \gets \log p(\wvec_{t} | \wvec^{(k)}_{1:t-1}, \cvec; \theta) + \log p(\wvec^{(k)}_{1:t-1}| \cvec;\theta) \]
  %  \item Save $K$ highest scoring target sequences
  %    \[\wvec_{1:t+1}^{(1:K)} \gets K\arg\max_{\wvec_t, k} s(\wvec_t, k)\]
  %  \end{enumerate}
  % \end{enumerate}
  \pause
  % \begin{itemize}
  % \item Note: Requires computing $p(\wvec_{t} | \wvec^{(k)}_{1:t-1}, \cvec; \theta)$ for many  $\wvec^{(k)}_{1:t-1}$
  % \end{itemize}
\end{frame}

% \begin{frame}[fragile]{Beam Search Example} ($K=3$)


%     \begin{center}
%   \begin{tikzpicture}[transform canvas = {scale=0.8}]
%     \tikzstyle{beam}=[draw, minimum height=0.6cm, anchor=base, text height=5, text depth=0, minimum width=1.5cm,thin, rounded corners, line width=0.03cm]
%    \tikzstyle{mat}=[draw=white]
%     \tikzset{>=stealth',every on chain/.append style={join},
%       every join/.style={->}}


%        \begin{scope}

%    \matrix (G) [matrix of nodes, nodes={beam},inner sep=1mm,row sep=0.03cm, column sep=0.8cm ] {
%     \node<1->(G-1-1){a}; & \node<2->(G-1-2){red}; & \node<3->(G-1-3){dog}; & \node<4->(G-1-4){smells}; & \node<5->(G-1-5){home};  & \node<6->(G-1-6){today}; \\
%     \node<1->(G-2-1){the}; & \node<2->(G-2-2){dog}; & \nodek<3->(G-2-3){dog}; & \node<4->(G-2-4){barks}; & \node<5->(G-2-5){quickly}; & \node<6->(G-2-6){Friday}; \\
%     \node<1->(G-3-1){red}; & \node<2->(G-3-2){blue}; & \node<3->(G-3-3){cat}; &  \node<4->(G-3-4){runs}; & \node<5->(G-3-5){straight}; & \node<6->(G-3-6){now}; \\    };

%     \only<2->{
%       \draw[->] (G-1-1.east) -> (G-1-2.west);
%       \draw[->] (G-2-1.east) -> (G-2-2.west);
%       \draw[->] (G-1-1.east) -> (G-3-2.west);
%       \draw[double, line width=0.03cm] (G-3-1.south west) -- (G-3-2.south east);
%     }

%     \only<3->{
%       \draw[->] (G-1-2.east) -> (G-2-3.west);
%       \draw[->] (G-3-2.east) -> (G-3-3.west);
%       \draw[->] (G-3-2.east) -> (G-1-3.west);
%       \draw[double, line width=0.03cm] (G-3-1.south west) -- (G-3-3.south east);
%     }

%  \only<4->{
%     \draw[->] (G-1-3.east) -> (G-3-4.west);
%     \draw[->] (G-2-3.east) -> (G-2-4.west);
%     \draw[->] (G-1-3.east) -> (G-1-4.west);
%     \draw[double, line width=0.03cm] (G-3-1.south west) -- (G-3-4.south east);
% }
%  \only<6->{
%     \draw[->] (G-1-5.east) -> (G-1-6.west);
%     \draw[->] (G-1-5.east) -> (G-3-6.west);
%     \draw[->] (G-2-5.east) -> (G-2-6.west);
%     \draw[double, line width=0.03cm] (G-3-1.south west) -- (G-3-6.south east);
% }

%  \only<5->{
%     \draw[->] (G-3-4.east) -> (G-1-5.west);
%     \draw[->] (G-3-4.east) -> (G-2-5.west);
%     \draw[->] (G-3-4.east) -> (G-3-5.west);
%     \draw[double, line width=0.03cm] (G-3-1.south west) -- (G-3-5.south east);
% }

% \end{scope}
% \end{tikzpicture}
%     \end{center}
%     \air

% For timesteps $t$ from  $1$ to $T$:
%    \begin{enumerate}
%    \item Compute for all $k, \wvec_{t}$
%      \[f(\wvec_t, \wvec_{1:t-1}^{(k)}) \gets \log p(\wvec_{t} | \wvec^{(k)}_{1:t-1}, \cvec) + \log p(\wvec^{(k)}_{1:t-1}| \cvec) \]
%    \item Replace the $K$ highest scoring target sequences
%      \[\wvec_{1:t}^{(1:K)} \gets K\argmax_{\wvec_{1:t}} (\wvec_t, \wvec_{1:t-1}^{(k)})\]
%    \end{enumerate}

% \end{frame}


\begin{frame}{Known Issues}

  \begin{center}
    Multiclass Training $\Rightarrow$ Structured Generation ?
  \end{center}

  \pause
  \begin{enumerate}
  \item Exposure Bias
    \begin{itemize}
    \item Training conditions on true history, but generation uses predicted history.
    \end{itemize}
    \air
    \pause

  \item Label Bias  %\cite{Lafferty2001}
    \begin{itemize}
    \item Score is locally multiclass, but  want to compare entire sequences.
    \end{itemize}
    \air
    \pause


  \item Metric Bias
    \begin{itemize}
    \item Training error is class accuracy, but evaluation uses n-gram match.
    \end{itemize}
  \end{enumerate}
  \pause
  \begin{center}
    \textbf{Strategy:} Modify model and training to target these issues.
  \end{center}
\end{frame}

% \begin{frame}{Beam Search Optimization}

%   \textbf{Strategy:} Modify training to target each issue.

%   \begin{itemize}
%   \item Exposure Bias, Label Bias, Metric Bias

%   \end{itemize}
% \air
% \air

%   \textbf{Applications:}

%   \begin{enumerate}
%   \item Improvements in training with less supervision.
%     \air
%   \item  Effective methods for downscaling translation models.
%     \air
%   \end{enumerate}
% \end{frame}


% \begin{frame}{\structure{Related Work:} Modify training data}
%   \air
%   \begin{itemize}
%   \item Data as Demonstrator \Cite{Venkatraman}, Scheduled Sampling \Cite{Bengio2015}
%   \end{itemize}
%   \air

%   \centerline{\structure{Related Work:} Use Reinforcement Learning}
%   \air
%   \begin{itemize}
%   \item MIXER \Cite{Ranzato2016}
%   \item Actor-Critic \Cite{Bahdanau2016}
%   \end{itemize}

%   \pause

%   Opinion:
%   \begin{itemize}
%   \item
%     DAD methods only address exposure bias,
%   \item RL is too strong a hammer .
%   \end{itemize}

%   % \begin{itemize}
%   % \item
%   % \end{itemize}
% \end{frame}

% \begin{frame}{Sequence-to-Sequence Learning as Beam Search Optimization}


%   Proposal: Directly modify the RNN training procedure to fix test biases.


%   % \begin{itemize}
%   % \item
%   % \end{itemize}
% \end{frame}

\begin{frame}{Modification 1: Beam Search at Training}

  \textbf{Fix:} Exposure Bias
    \begin{itemize}
    \item Take prediction algorithm into account during training.
    \end{itemize}
    \air
    \pause

   \begin{center}
     \includegraphics[width=8cm, clip, trim={0cm 3cm 0cm 1cm}]{out-4}
   \end{center}
   \begin{itemize}
   \item Run our beam search procedure during training (structured training)
     \air

   \item Loss tied to  mistakes, e.g. compare true sequence \textcolor{gold}{$\hat{y}_{1:t}$}  to \textcolor{red}{$y^{(K)}_{1:t}$ } worst in beam
   \end{itemize}
   % \begin{enumerate}
   % \item Compute the score of every possible next word.

   %   \[f(y_t, y_{1:t-1}^{(k)}) \gets \log p(y_{t}\ |\ y^{(k)}_{1:t-1}, x) + \log p(y^{(k)}_{1:t-1} \ |\  x) \]

   % % \only<2>{
   % %   \[f(y_t, y_{1:t-1}^{(k)})  \gets  \structure{f(y_t, y_{1:t-1}^{(k)}, x; \theta)} \]}

   % \item Prune to only the $K$ highest-scoring,
   %   \[y_{1:t}^{(1:K)} \gets K\argmax_{y_{1:t}} f(y_t, y_{1:t-1}^{(k)})\]
   % \end{enumerate}
  % \begin{enumerate}
  % % \item Start with $K$ partial starting hypotheses $\wvec^{(1:K)}$
  % % \item For timesteps $t$ from  $1$ to $T$:
  %  \begin{enumerate}
  %  \item Compute for all $k, \wvec_{t}$
  %    \only<1>{
  %    \[f(\wvec_t, \wvec_{1:t-1}^{(k)}) \gets \alert{\log p(\wvec_{t} | \wvec^{(k)}_{1:t-1}, \cvec; \theta) + \log p(\wvec^{(k)}_{1:t-1}| \cvec;\theta)} \]
  %  }
  %  \only<2>{
  %    \[f(\wvec_t, \wvec_{1:t-1}^{(k)})  \gets  \structure{f(\wvec_t, \wvec_{1:t-1}^{(k)}, \cvec; \theta)} \]
  %  }
  %  \item Replace the  $K$ highest scoring target sequences
  %    \[\wvec_{1:t}^{(1:K)} \gets K\argmax_{\wvec_{1:t}} s(\wvec_t, \wvec_{1:t-1}^{(k)})\]
  %  \end{enumerate}
  % \end{enumerate}
\end{frame}

\begin{frame}{Modification 2: Global Scoring Function  }
  \textbf{Fix:}  Label Bias
    \begin{itemize}
    \item Use a global sequence scoring function.
    \end{itemize}
    \pause

  % \begin{center}
  %   % \structure{(Idea 1)} Replace local softmax with sequence scorer
  %   % \air
  %
  % \end{center}

    \air

  % \textbf{Proposed Fix:}
  \begin{center}
    $f_{\theta}(y_{1:t}, x)= \mathbf{W} [\mathbf{h}_{t-1}; \mathbf{c}]$

    \includegraphics[width=0.5\textwidth, trim={0cm 18cm 0cm 0cm}, clip]{rnnlm6}
  \end{center}


  \begin{itemize}
  \item Replace local $ \log p(y_{t} | y_{1:t-1}, x; \theta)$ with
    a global scoring model $f_{\theta}(y_{1:t}, x)$.
  \end{itemize}
  \air



\end{frame}



\begin{frame}{Modification 3: Train with Margin}

  \textbf{Fix:} Metric Bias
  \begin{itemize}
  \item Incorporate a metric specific term, e.g. n-gram mismatch
  \end{itemize}
  \air
  \pause

  \[ \mathcal{L}(\theta) = \sum_{t} \Delta(\textcolor{gold}{\hat{y}_{1:t}}, \textcolor{red}{y_{1:t}^{(K)}}) \left[1 - f_{\theta}( \textcolor{gold}{\hat{y}_{1:t}}, x) +  f_{\theta}(\textcolor{red}{y_{1:t}^{(K)}}, x) \right]_{+} \]

  \begin{itemize}
  \item Positive if true sequence $\textcolor{gold}{\hat{y}_{1:t}}$ within margin of worst beam sequence $\textcolor{red}{y^{(K)}_{1:t}}$.
  \item Slack-rescaled margin takes problem-specific $\Delta$ into account.
  \end{itemize}

\end{frame}

\begin{myverbbox}{\sTW}
 { \cal K } ^ { L } ( \sigma = 2 ) = \left( \begin{array}
{ c c } { - \frac { d ^ { 2 } } { d x ^ { 2 } } + 4 - \frac
 { 3 } { \operatorname { c o s h } ^ { 2 } x } } \& { \frac
 { 3 } { d x ^ { 2 } } }  { \frac { 3 } { \operatorname
 { c o s h } ^ { 2 } x } } \& { - \frac { d ^ { 2 } }
{ d x ^ { 2 } } + 4 - \frac { 3 } { \operatorname { c o s h }
^ { 2 } x } }  \end{array} \right) \qquad
\end{myverbbox}

% \begin{frame}{Extension: Training with Hard Constraints}

%   Beam Search Optimization supports hard constraints at training.

%   \textbf{Example:} Code generation with a known grammar,
%   \air
%     \begin{center}
%       \begin{tikzpicture}
%         \node (b)[xshift=6cm, rectangle, scale=0.6,
%         draw,thick,fill=blue!0,text width=32em, rounded corners, inner
%         sep =5pt, minimum height=1em]{\baselineskip=50pt \footnotesize
%           \sTW };

%     \end{tikzpicture}
%   \end{center}

% \end{frame}


\begin{frame}[fragile]{Standard Training Example}
  \textcolor{gold}{True}: ground-truth training sequence $\hat{y}_{1:T}$.

  \begin{center}
  \begin{tikzpicture}[transform canvas = {scale=0.8}]
  \tikzstyle{beam}=[draw, minimum height=0.6cm, anchor=base, text height=5, text depth=0, minimum width=1.5cm,thin, rounded corners, line width=0.03cm]
  \tikzstyle{mat}=[draw=white]
\tikzset{>=stealth',every on chain/.append style={join},
         every join/.style={->}}
       \begin{scope}[]

      \matrix (G) [matrix of nodes,nodes={beam}, inner sep=1mm,row sep=0.06cm,column sep=0.8cm ] {
        \node[fill=yellow](G-1-1){a}; & \node<2->[fill=yellow](G-1-2){red}; & \node<3->[fill=yellow](G-1-3){dog}; & \node<3->[fill=yellow](G-1-4){runs}; & \node<3->[fill=yellow](G-1-5){quickly}; & \node<3->[fill=yellow](G-1-6){today}; \\
         % & \node[fill=lightgray](G-2-2){\textcolor{blue}{blue}}; & \node[fill=lightgray](G-2-3){\textcolor{blue}{dog}}; & \node[fill=lightgray](G-2-4){\textcolor{blue}{barks}}; & \node[fill=lightgray](G-2-5){\textcolor{red}{home}}; & \node[fill=lightgray](G-2-6){\textcolor{red}{today}}; \\
      };
    \draw<2->[->] (G-1-1.east) -> (G-1-2.west);
    \draw<3->[->] (G-1-2.east) -> (G-1-3.west);
    \draw<3->[->] (G-1-3.east) -> (G-1-4.west);
    \draw<3->[->] (G-1-4.east) -> (G-1-5.west);
    \draw<3->[->] (G-1-5.east) -> (G-1-6.west);

    % \draw[->] (G-1-1.east) -> (G-2-2.west);
    % \draw[->] (G-2-2.east) -> (G-2-3.west);
    % \draw[->] (G-2-3.east) -> (G-2-4.west);
    % \draw[->] (G-1-4.east) -> (G-2-5.west);
    % \draw[->] (G-2-5.east) -> (G-2-6.west);
    \end{scope}
  \end{tikzpicture}
  \air
  \[ {\cal L}(\theta) = - \log p(\text{a}\ |\  x; \theta) \pause - \log p(\text{red}\ |\  \text{a}, x; \theta) \pause- \log p(\text{dog}\ |\  \text{a \ red}, x; \theta) \pause - \ldots\]
  \end{center}

\end{frame}

\begin{frame}[fragile]{Beam Search Training Example}
  \begin{center}
    \begin{center}

    \end{center}

 % \textcolor{gold}{True}: ground-truth training sequence $\hat{y}_{1:t}$

    \air
    \air
    \air


  % \item<4-> \textcolor{red}{Predicted}: lowest-scoring violating sequence  $y_{1:t}^{(K)}$
  % \end{itemize}

  \begin{tikzpicture}[transform canvas = {scale=0.8}]
  \tikzstyle{beam}=[draw, minimum height=0.6cm, anchor=base, text height=5, text depth=0, minimum width=1.5cm,thin, rounded corners, line width=0.03cm]
  \tikzstyle{mat}=[draw=white]
\tikzset{>=stealth',every on chain/.append style={join},
         every join/.style={->}}


       % \node[draw = white, yshift=1.8cm]{Time Step};
       %\node[draw = white, xshift=-7.5cm, yshift=0.5cm]{Beam:};
       \begin{scope}


   \matrix (G) [matrix of nodes, nodes={beam},inner sep=1mm,row sep=0.03cm, column sep=0.8cm ] {
    \node<1->[fill=yellow](G-1-1){\textcolor{black}{a}}; \node<4->[fill=yellow](G-1-1){\textcolor{black}{a}}; & \node<2->[fill=yellow](G-1-2){red}; \node<4->[fill=yellow](G-1-2){red}; & \node<3->[](G-1-3){\textcolor{black}{dog}}; \node<4->[fill=red!20](G-1-3){\textcolor{black}{dog}}; & \node<4->(G-1-4){smells}; & \node<5->[](G-1-5){{home}}; \node<6->[fill=red!20](G-1-5){{home}};  & \node<6->[](G-1-6){{today}}; \\
    \node<1->(G-2-1){the}; & \node<2->(G-2-2){dog}; & \node<3->[fill=yellow](G-2-3){dog}; & \node<4->(G-2-4){barks}; & \node<5->[fill=yellow](G-2-5){quickly}; & \node<6->(G-2-6){Friday}; \\
    \node<1->(G-3-1){red}; & \node<2->[](G-3-2){\textcolor{black}{blue}}; \node<4->[fill=red!20](G-3-2){\textcolor{black}{blue}}; & \node<3->(G-3-3){cat}; &  \node<4->[fill=red!20](G-3-4){\textcolor{black}{barks}}; & \node<5->(G-3-5){straight}; & \node<6->[fill=red!20](G-3-6){now}; \\
    & & & \node<4->[fill=yellow](G-4-4){runs}; & & \node<6->[fill=yellow](G-4-6){today}; \\
    };

    \only<2->{
      \draw[->] (G-1-1.east) -> (G-1-2.west);
      \draw[->] (G-2-1.east) -> (G-2-2.west);
      \draw[->] (G-1-1.east) -> (G-3-2.west);
      \draw[double, line width=0.03cm] (G-3-1.south west) -- (G-3-2.south east);
    }

    \only<3->{
      \draw[->] (G-1-2.east) -> (G-2-3.west);
      \draw[->] (G-3-2.east) -> (G-3-3.west);
      \draw[->] (G-3-2.east) -> (G-1-3.west);
      \draw[double, line width=0.03cm] (G-3-1.south west) -- (G-3-3.south east);
    }

 \only<4->{
    \draw[->] (G-2-3.east) -> (G-4-4.west);
    \draw[->] (G-1-3.east) -> (G-3-4.west);
    \draw[->] (G-2-3.east) -> (G-2-4.west);
    \draw[->] (G-1-3.east) -> (G-1-4.west);
    \draw[double, line width=0.03cm] (G-3-1.south west) -- (G-3-4.south east);
}
 \only<6->{
    \draw[->] (G-1-5.east) -> (G-1-6.west);
    \draw[->] (G-1-5.east) -> (G-3-6.west);
    \draw[->] (G-2-5.east) -> (G-2-6.west);
    \draw[->] (G-2-5.east) -> (G-4-6.west);
    \draw[double, line width=0.03cm] (G-3-1.south west) -- (G-3-6.south east);
}

    % \draw[->] (G-3-2.east) -> (G-3-3.west);
    % \draw[->] (G-3-2.east) -> (G-1-3.west);

 \only<5->{
    \draw[->, dashed] (G-4-4.east) -> (G-1-5.west);
    \draw[->, dashed] (G-4-4.east) -> (G-2-5.west);
    \draw[->, dashed] (G-4-4.east) -> (G-3-5.west);
    \draw[double, line width=0.03cm] (G-3-1.south west) -- (G-3-5.south east);
}

       \end{scope}
    % \draw(G-1-1.north east) rectangle (G-3-1.south west);
    % \draw(G-1-2.north east) rectangle (G-3-2.south west);

    % \begin{scope}[yshift=-2.8cm]

    %   \matrix (G) [matrix of nodes,nodes={beam}, inner sep=1mm,row sep=0.06cm,column sep=0.8cm ] {
    %     \node[fill=yellow](G-1-1){a}; & \node[fill=yellow](G-1-2){red}; & \node[fill=yellow](G-1-3){dog}; & \node[fill=yellow](G-1-4){runs}; & \node[fill=yellow](G-1-5){quickly}; & \node[fill=yellow](G-1-6){today}; \\
    %      & \node[fill=lightgray](G-2-2){\textcolor{blue}{blue}}; & \node[fill=lightgray](G-2-3){\textcolor{blue}{dog}}; & \node[fill=lightgray](G-2-4){\textcolor{blue}{barks}}; & \node[fill=lightgray](G-2-5){\textcolor{red}{home}}; & \node[fill=lightgray](G-2-6){\textcolor{red}{today}}; \\
    %   };
    % \draw[->] (G-1-1.east) -> (G-1-2.west);
    % \draw[->] (G-1-2.east) -> (G-1-3.west);
    % \draw[->] (G-1-3.east) -> (G-1-4.west);
    % \draw[->] (G-1-4.east) -> (G-1-5.west);
    % \draw[->] (G-1-5.east) -> (G-1-6.west);

    % \draw[->] (G-1-1.east) -> (G-2-2.west);
    % \draw[->] (G-2-2.east) -> (G-2-3.west);
    % \draw[->] (G-2-3.east) -> (G-2-4.west);
    % \draw[->] (G-1-4.east) -> (G-2-5.west);
    % \draw[->] (G-2-5.east) -> (G-2-6.west);

    % \end{scope}
\end{tikzpicture}
  \end{center}
  \air
  \air
  \air
% \begin{align*}
%  \mathcal{L}&(\theta) = \sum_{t} \Delta(\wvec_{1:t}, \wvec_{1:t}^{(K)}) \left[1 - f(\wvec_t, \wvec_{1:t-1}, \cvec) +  f(\wvec_t^{(K)}, \wvec_{1:t-1}^{(K)}, \cvec) \right]
% \end{align*}

  \begin{eqnarray*}
 {\cal L}(\theta) &=& 0 \pause + 0 \pause +  0 \pause +  \Delta \left[1 - f_{\theta}( \textcolor{gold}{\text{a red dog runs}}, x) +  f_{\theta}(\textcolor{red}{\text{a blue dog barks}}, x) \right] \pause + 0 \\
            & &   \pause + \Delta \left[1 - f_{\theta}( \textcolor{gold}{\text{a red dog runs quickly today}}, x) +  f_{\theta}(\textcolor{red}{\text{a red dog runs home now}}, x) \right]
  \end{eqnarray*}

  % Strategy: if true falls off beam, restart from ground truth (learning as search optimization \cite{daume05learning})
\end{frame}

\begin{frame}[fragile]{Parameter Updates: Structured Backpropagation}
  \begin{center}
  \begin{eqnarray*}
 {\cal L}(\theta) &=&   \Delta \left[1 - f_{\theta}( \textcolor{gold}{\text{a red dog runs}}, x) +  f_{\theta}(\textcolor{red}{\text{a blue dog barks}}, x) \right]   \\
            & &    + \Delta \left[1 - f_{\theta}( \textcolor{gold}{\text{a red dog runs quickly today}}, x) +  f_{\theta}(\textcolor{red}{\text{a red dog runs home now}}, x) \right]
  \end{eqnarray*}


  \air
  \air



  \begin{tikzpicture}[transform canvas = {scale=0.8}]
    \tikzstyle{beam}=[draw, minimum height=0.6cm, anchor=base, text height=5, text depth=0, minimum width=1.5cm,thin, rounded corners, line width=0.03cm]
    \tikzstyle{mat}=[draw=white]
    \tikzset{>=stealth',every on chain/.append style={join},
      every join/.style={->}}


       % \node[draw = white, yshift=1.8cm]{Time Step};
       %\node[draw = white, xshift=-7.5cm, yshift=0.5cm]{Beam:};
   %     \begin{scope}


   % \matrix (G) [matrix of nodes, nodes={beam},inner sep=1mm,row sep=0.03cm, column sep=0.8cm ] {
   %   \node[fill=yellow](G-1-1){\textcolor{blue}{a}}; & \node[fill=yellow](G-1-2){red}; & \node[fill=lightgray](G-1-3){\textcolor{blue}{dog}}; & \node(G-1-4){smells}; & \node[fill=lightgray](G-1-5){\textcolor{red}{home}};  & \node[fill=lightgray](G-1-6){\textcolor{red}{today}}; \\
   %   \node(G-2-1){the}; & \node(G-2-2){dog}; & \node[fill=yellow](G-2-3){dog}; & \node(G-2-4){barks}; & \node[fill=yellow](G-2-5){quickly}; & \node(G-2-6){Friday}; \\
   %   \node(G-3-1){red}; & \node[fill=lightgray](G-3-2){\textcolor{blue}{blue}}; & \node(G-3-3){cat}; &  \node[fill=lightgray](G-3-4){\textcolor{blue}{barks}}; & \node(G-3-5){straight}; & \node[](G-3-6){now}; \\
   %   & & & \node[fill=yellow](G-4-4){runs}; & & \node[fill=yellow](G-4-6){today}; \\
   %  };


   %  \draw[->] (G-1-1.east) -> (G-1-2.west);
   %  \draw[->] (G-2-1.east) -> (G-2-2.west);
   %  \draw[->] (G-1-1.east) -> (G-3-2.west);
   %  \draw[double, line width=0.03cm] (G-3-1.south west) -- (G-3-2.south east);


   %  \draw[->] (G-1-2.east) -> (G-2-3.west);
   %  \draw[->] (G-3-2.east) -> (G-3-3.west);
   %  \draw[->] (G-3-2.east) -> (G-1-3.west);
   %  \draw[double, line width=0.03cm] (G-3-1.south west) -- (G-3-3.south east);

   %  \draw[->] (G-2-3.east) -> (G-4-4.west);
   %  \draw[->] (G-1-3.east) -> (G-3-4.west);
   %  \draw[->] (G-2-3.east) -> (G-2-4.west);
   %  \draw[->] (G-1-3.east) -> (G-1-4.west);
   %  \draw[double, line width=0.03cm] (G-3-1.south west) -- (G-3-4.south east);

   %  \draw[->] (G-1-5.east) -> (G-1-6.west);
   %  \draw[->] (G-1-5.east) -> (G-3-6.west);
   %  \draw[->] (G-2-5.east) -> (G-2-6.west);
   %  \draw[->] (G-2-5.east) -> (G-4-6.west);
   %  \draw[double, line width=0.03cm] (G-3-1.south west) -- (G-3-6.south east);


   %  % \draw[->] (G-3-2.east) -> (G-3-3.west);
   %  % \draw[->] (G-3-2.east) -> (G-1-3.west);


   %  \draw[->, dashed] (G-4-4.east) -> (G-1-5.west);
   %  \draw[->, dashed] (G-4-4.east) -> (G-2-5.west);
   %  \draw[->, dashed] (G-4-4.east) -> (G-3-5.west);
   %  \draw[double, line width=0.03cm] (G-3-1.south west) -- (G-3-5.south east);


   %     \end{scope}


       \begin{scope}

      \matrix (G) [matrix of nodes,nodes={beam}, inner sep=1mm,row sep=0.06cm,column sep=0.8cm ] {
        \node[fill=yellow](G-1-1){a}; & \node[fill=yellow](G-1-2){red}; & \node[fill=yellow](G-1-3){dog}; & \node[fill=yellow](G-1-4){runs}; & \node[fill=yellow](G-1-5){quickly}; & \node[fill=yellow](G-1-6){today}; \\
         & \node[fill=lightgray](G-2-2){\textcolor{blue}{blue}}; & \node[fill=lightgray](G-2-3){\textcolor{blue}{dog}}; & \node[fill=lightgray](G-2-4){\textcolor{blue}{barks}}; & \node[fill=lightgray](G-2-5){\textcolor{red}{home}}; & \node[fill=lightgray](G-2-6){\textcolor{red}{now}}; \\
      };
    \draw[->] (G-1-1.east) -> (G-1-2.west);
    \draw[->] (G-1-2.east) -> (G-1-3.west);
    \draw[->] (G-1-3.east) -> (G-1-4.west);
    \draw[->] (G-1-4.east) -> (G-1-5.west);
    \draw[->] (G-1-5.east) -> (G-1-6.west);

    \draw[->] (G-1-1.east) -> (G-2-2.west);
    \draw[->] (G-2-2.east) -> (G-2-3.west);
    \draw[->] (G-2-3.east) -> (G-2-4.west);
    \draw[->] (G-1-4.east) -> (G-2-5.west);
    \draw[->] (G-2-5.east) -> (G-2-6.west);

    \end{scope}
  \end{tikzpicture}
  \end{center}
  \pause
\air

  Key Modifications:

  \begin{itemize}
  \item Modification 1: ${\cal L}(\theta)$, loss determined by beam search procedure.
  \item Modification 2: $f(y_{1:t}, x; \theta)$, score determined by global model.
  \item Modification 3:  $\Delta$, scales loss by n-gram metric.
  \end{itemize}

  % \begin{itemize}
  % \item Margin gradients are sparse, only grey sequences get updates.
  % \item Backprop as efficient as standard models.
  % \end{itemize}
\end{frame}

% \begin{frame}
%   \begin{center}
%     \structure{(Idea 3)} Extension: Incorporate hard constraints at training.
%   \end{center}

% \end{frame}

% \begin{frame}{Theoretical Issues Revisited}
%   \begin{itemize}

%   \item Exposure Bias
%     \begin{itemize}
%     \item Beam search at training
%     \end{itemize}
%     \air
%   \item Train/Test Loss Mismatch
%     \begin{itemize}
%     \item Slack-rescaled margin can capture correct loss.
%     \end{itemize}

%     \air
%   \item Label Bias  \cite{Lafferty2001}
%     \begin{itemize}
%     \item Sequence regression is not locally normalized
%     \end{itemize}

%   \end{itemize}
% \end{frame}


% \begin{frame}{Experiments}
%   \air

%   Experiments run on three different seq2seq baseline tasks

%   \begin{itemize}
%   \item Word Ordering
%     \air
%   \item Dependency Parsing
%     \air
%   \item Machine Translation
%   \end{itemize}



%   Details:
%   \begin{itemize}
%   \item Utilize our \textit{seq2seq-attn} code, very strong attention-based system
%   \item Pretrained with NLL.
%   \item Trained with a curriculum to gradually increase beam size.
%   \end{itemize}

% \end{frame}


\begin{frame}{Results}
  \vspace{-0.2cm}
  \begin{table}
  \centering
    \footnotesize
  \begin{tabular}{lcc}
    \toprule
    Train Beam & $K$ = 5 & $K$ = 10 \\
    \midrule
     & \multicolumn{2}{c}{Word Ordering (BLEU) } \\
    \midrule
    Encoder-Decoder & 29.8 & 31.0 \\
    % Beam Search Optimization     & 33.2 & 34.3 \\
    Beam Search Optimization &  \textbf{34.3} & \textbf{34.5} \\
    \midrule

%   \end{tabular}
%   \label{tab:wo}
% \end{table}


% \begin{table}
%   \centering
%   \hspace*{-0.3cm}\begin{tabular}{lccc}
    % \toprule
    % & \multicolumn{3}{c}{Dependency Parsing (UAS) } \\
    % \midrule
    % Encoder-Decode & \textbf{87.33} & 88.53 & 88.66\\
    % Beam Search Optimization & 86.91 & 91.00 & 91.17 \\
    % Beam Search Optimization-Constraints & 85.11 & \textbf{91.25} & \textbf{91.57} \\
    % \midrule
    % Andor & 93.17/91.18 & - & - \\
    % \bottomrule

    \midrule

    & \multicolumn{2}{c}{IWSLT Machine Translation (BLEU) } \\
    % &  $K_e$ = 1 & $K_e$ = 5 & $K_e$ = 10 \\
    \midrule
    Encoder-Decoder & 24.0 & 23.9 \\
    Beam-Search Optimization, $\Delta$  &  \textbf{26.4} & \textbf{25.5} \\
    % \midrule
    % XENT & 17.74 & $\leq$ 20.5 & $\leq$ 20.5 \\
    % DAD & 20.12 & $\leq$ 22.5 & $\leq$ 23.0 \\
    % MIXER & 20.73 & - & $\leq$ 22.0 \\
    \bottomrule
  \end{tabular}
  \label{tab:mtfinal}
\end{table}

\end{frame}


% \begin{frame}

% \end{frame}

% \begin{frame}
%   \centerline{\structure{Related Work: Compressing Deep Models}}
% \air

% \begin{itemize}
% \item \textbf{Pruning}: Prune weights based on importance criterion
% \cite{LeCun1990,Han2016}
% \item \textbf{Knowledge Distillation}: Train a \textit{student} model to learn
% from a \textit{teacher} model \cite{Bucila2006,Ba2014,Hinton2015}.
% \end{itemize}
% \air
% Other methods:
% \begin{itemize}
% \item low-rank matrix factorization of weight matrices \cite{Denton2014}
% \item weight binarization \cite{Lin2016}
% \item weight sharing \cite{Chen2015}
% \end{itemize}
% \end{frame}



% \begin{frame}
% \centerline{\structure{Word-Level Knowledge Distillation}}
% \air
% \air

% \begin{columns}
% \begin{column}{6.5cm}
% Teacher network: $q(\yvec \given  ; \theta_T$)  \\
% \air
% \air
% Student network: $p(\yvec \given \xvec ; \theta$)
% \end{column}
% \begin{column}{5.5cm}
% \includegraphics[width=5.5cm]{word-kd-1}
% \end{column}
% \end{columns}
% \end{frame}

% \begin{frame}
%   (Teacher-Student Diagram)
% \end{frame}

% \begin{frame}{Application: Model Compression}
%   \research{\cite{Kim2016a}}
%  Knowledge Distillation: Train a \textit{student} model to learn
% from a \textit{teacher} model. %\cite{Bucila2006,Ba2014,Hinton2015}.

% \begin{center}
%   \includegraphics[width=10cm]{distill}
% \end{center}



% % \air
% % Other methods:
% % \begin{itemize}
% % \item low-rank matrix factorization of weight matrices \cite{Denton2014}
% % \item weight binarization \cite{Lin2016}
% % \item weight sharing \cite{Chen2015}
% % \end{itemize}
% % \end{frame}



% \end{frame}

% \begin{frame}{Baseline}

% \begin{columns}
% \begin{column}{7cm}
% Minimize :
% \air
% $${\cal L}(\theta)-\sum_t \log p(y_t=\hat{y}_t \given \hat{y}_{1:t-1}, x ; \theta)$$

% where $\hat{y}_t$ is the ground truth word at time $t$.

% \air

% Cross-entropy with ground truth.

% \end{column}
% \begin{column}{8cm}
% \includegraphics[trim={0.5cm 0.5cm 0.5cm 0.5cm}, clip, width=6cm]{word-kd-0}
% \end{column}
% \end{columns}
% \end{frame}

% \begin{frame}
% \centerline{\structure{Word-Level Knowledge Distillation}}
% \air
% \air
% \begin{columns}
% \begin{column}{6.5cm}
% Teacher network: $q(y_{t} | y_{1:t-1}, \cvec  ; \theta_T$)  \\
% \air
% \air
% Student network: $p(y_{t} | y_{1:t-1}, \cvec; \theta$)
% \end{column}
% \begin{column}{5.5cm}
% \includegraphics[width=5.5cm]{word-kd-1}
% \end{column}
% \end{columns}
% \end{frame}


\begin{frame}{Application: Sequence Compression }
  \research{\cite{Kim2016a}}
  \begin{center}
    \includegraphics[trim={0.2cm 0.2cm 0.2cm 3cm}, clip,
    width=7cm]{word-kd-2}
  \end{center}
\end{frame}

\begin{frame}{Application: Sequence Compression }
  \research{\cite{Kim2016a}}
  \begin{center}
    \includegraphics[trim={0.2cm 0.2cm 0.2cm 0.2cm}, clip,
    width=7cm]{seq-kd-2}
  \end{center}
\end{frame}


% \begin{frame}
% \centerline{\structure{Word-Level Knowledge Distillation}}
% \air
% \begin{columns}
% \begin{column}{6.5cm}
% Add a term for NLL (equivalent to minimizing cross-entropy between student a mixture distribution of teacher/data distributions)
% \air
% $$\mathcal{L}(\theta) = \alpha\mathcal{L}_{\text{WORD-KD}}(\theta) + (1-\alpha)\mathcal{L}_{\text{NLL}}(\theta)$$
% \air
% $\alpha$ is a hyperparameter (we use $\alpha = 0.5$)
% \end{column}

% \begin{column}{5.5cm}
% \includegraphics[width=5.5cm]{word-kd-3}
% \end{column}
% \end{columns}
% \end{frame}

% \begin{frame}{Sequence-Level Knowledge Distillation}
% \air
% \air

% \textbf{Motivation:} Replace multiclass with sequence-level cross-entropy.

% \begin{align*}
% \mathcal{L}_{\text{WORD-KD}}(\theta) = -\sum_t \sum_v &q(y_t=v \given \hat{y}_{1: t-1}, x ; \theta_T)\times  \log p(y_t =v \given \hat{y}_{1: t-1}, \cvec ; \theta)
% \end{align*}
% \[ \Downarrow \]
% \[ \mathcal{L}_{\text{SEQ-KD}}(\theta) = -\sum_{v_1} \ldots \sum_{v_T} q(y_{1:T}= v_{1:T}  | x; \theta_T) \times \log p(y_{1:T}=v_{1:T} | x ; \theta)
% \]

% Mimic sequence output of teacher model.
% \air

% Note: bottom distribution is again intractable.


% \end{frame}

% \begin{frame}{Sequence Heuristic Approximation}
% \air
% \air

% %\centerline{Sequence-Level Knowledge Distillation}
% \begin{columns}
% \begin{column}{6cm}
% Approximate $q(y_{1:T} \given x )$ with (beam search) mode sample
% $$q(y_{1:T} \given x ) \approx \mathbf{1}\{\argmax_{y} q(y_{1:T} \given x )\}$$
% % \air
% % $$ y^*_{1:T} \approx  \argmax_{y_{1:T}} q(y_{1:T} \given x ) $$
% % \\
% % Empirically, point estimate captures
% % significant mass

% \end{column}
% \begin{column}{8cm}
% \includegraphics[trim={0.5cm 0.5cm 0.5cm 0.5cm}, clip, width=8cm]{seq-kd-1}
% \end{column}
% \end{columns}
% \end{frame}

% \begin{frame}{Sequence-Level Knowledge Distillation}

% % \begin{columns}
% % \begin{column}{7cm}
% % \begin{align*}
% % \mathcal{L}_\text{SEQ-KD}(\theta) &= -\log p(y^*_{1:T} \given x ; \theta)  \\
% % & \hspace*{-1cm} \displaystyle \approx   -\sum_{v_{1:T}} q(y_{1:T}=v_{1:t}|x; \theta_T) \log p(y_{1:T} | x ; \theta)
% % \end{align*}
% % \air

% % Extension: $\mathcal{L}_\text{SEQ-INTER}(\theta)$ select sample based on ground truth $\hat{y}$
% % as well.
% % \end{column}
% % \begin{column}{7cm}
%   \begin{center}
%     \includegraphics[trim={0.2cm 0.2cm 0.2cm 0.2cm}, clip,
%     width=7cm]{seq-kd-2}
%   \end{center}
% % \end{column}
% % \end{columns}
% \end{frame}

% \begin{frame}{Results: WMT English $\rightarrow$ German Translation}
% \air
% \air
% \begin{table}
% \centering
% \begin{tabular}{lccccrr}
% \toprule
% Model &    BLEU$_{K=1}$   & $\Delta_{K=1}$ & BLEU$_{K=5}$ & $\Delta_{K=5}$ \\%& PPL & $p(y^*)$ \\
% \midrule
% $4 \times 1000$ \\
% Teacher    & $17.7$ &  $-$ & $19.5$&   $-$ \\%&   $6.7$ &  $1.3\%$ \\
% \midrule
% \pause
% $2 \times 500$ \\
%   Student  $\,$   & $14.7$ & $-$ & $17.6$&  $-$ \\%&  $8.2$ & $0.9\%$  \\
%   \pause
% \hspace{1mm} Word-KD  & $15.4$ & $+0.7$& $17.7$& $+0.1$\\%&  $8.0$ & $1.0\%$  \\
% \pause
% % \hspace{1mm} Seq-KD   & $18.9$ & $+\mathbf{4.2}$& $19.0$& $+1.4$\\%&  $22.7$ & $                     \pause                                                         16.9\%$ \\
% \hspace{1mm} Seq-KD  & $18.9$ & $+\mathbf{4.2}$&$19.3$ & $+\mathbf{1.7}$ \\ %&  $15.8$ & $7.6\%$  \\
% % \midrule
% % \pause
% % $4 \times 1000$ \\
% % \hspace{1mm} Seq-Inter    & $19.6$ & $+1.9$&  $19.8$& $+0.3$\\ %&   $10.4$ & $8.2\%$   \\
% \bottomrule
% \end{tabular}

% \end{table}
% \air
% \air
% \end{frame}

% \begin{frame}{Combining Knowledge Distillation and Pruning}
% \air
% \air
% \centering
% \includegraphics[width=0.8\textwidth]{modelsize}
% \end{frame}


\begin{frame}{Scaling Translation Models}

  \begin{center}
    \movie[width=\textwidth, repeat, height=0.85\textheight, width=\textwidth, poster, showcontrols]{Scaling Video}{videos/translate.mp4}
  \end{center}
\end{frame}

% \begin{frame}{WNMT Translation Scaling Shared Task 2018}
%   (Results)
% \end{frame}
